{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f827039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "import yaml\n",
    "from torch.utils import data\n",
    "# ê°œë³„ json ë¼ë²¨ íŒŒì¼ì„ ì´ìš©í•´ í•™ìŠµ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from nets import nn\n",
    "from utils import util\n",
    "from utils.dataset import Dataset\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76e71dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ íŒŒë¼ë¯¸í„°:\n",
      "í´ë˜ìŠ¤ ì´ë¦„: {0: 'nucleus'}\n",
      "í´ë˜ìŠ¤ ìˆ˜: 1\n",
      "ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì—ì„œ ì°¾ì€ ë¼ë²¨ íŒŒì¼: 135ê°œ\n",
      "ë‘ ë²ˆì§¸ ë°ì´í„°ì…‹ì—ì„œ ì°¾ì€ ë¼ë²¨ íŒŒì¼: 344ê°œ\n",
      "\n",
      "ì´ ë°ì´í„°:\n",
      "ì´ë¯¸ì§€ íŒŒì¼: 479ê°œ\n",
      "ë¼ë²¨ ì„¸íŠ¸: 479ê°œ\n",
      "\n",
      "ë¼ë²¨ ë¶„í¬:\n",
      "ì´ ë¼ë²¨ ìˆ˜: 929007\n",
      "í´ë˜ìŠ¤ 1: 929007ê°œ\n",
      "ë¹ˆ ë¼ë²¨ ì„¸íŠ¸: 2ê°œ\n",
      "Warning: ë¹ˆ ë¼ë²¨ ì„¸íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” lossê°€ 0ì´ ë˜ëŠ” ì›ì¸ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# íŒŒë¼ë¯¸í„° ë° ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "with open('utils/Binary_args.yaml', errors='ignore') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "print(\"ëª¨ë¸ íŒŒë¼ë¯¸í„°:\")\n",
    "print(f\"í´ë˜ìŠ¤ ì´ë¦„: {params['names']}\")\n",
    "print(f\"í´ë˜ìŠ¤ ìˆ˜: {len(params['names'])}\")\n",
    "\n",
    "label_dir = '../../data/IGNITE/annotations/pdl1/binary_individual/'\n",
    "image_dir = '../../data/IGNITE/images/pdl1/nuclei/'\n",
    "\n",
    "label_files = sorted(glob.glob(os.path.join(label_dir, '*.json')))\n",
    "filenames = []\n",
    "labels = []\n",
    "\n",
    "print(f\"ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì—ì„œ ì°¾ì€ ë¼ë²¨ íŒŒì¼: {len(label_files)}ê°œ\")\n",
    "\n",
    "for label_file in label_files:\n",
    "    with open(label_file) as f:\n",
    "        data1 = json.load(f)\n",
    "    img_path = os.path.join(image_dir, data1['image']['file_name'])\n",
    "    if os.path.exists(img_path):\n",
    "        filenames.append(img_path)\n",
    "        temp_labels = []\n",
    "        for i in range(len(data1['annotations'])):\n",
    "            # ëª¨ë“  nucleusë¥¼ í´ë˜ìŠ¤ 1ë¡œ í†µì¼\n",
    "            temp_labels.append([1, int(data1['annotations'][i]['bbox'][0]),\n",
    "                         int(data1['annotations'][i]['bbox'][1]),int(data1['annotations'][i]['bbox'][2]),int(data1['annotations'][i]['bbox'][3])])\n",
    "            \n",
    "        labels.append(temp_labels)\n",
    "        \n",
    "label_dir = '../../data/IGNITE/annotations/pdl1/individual/'\n",
    "image_dir = '../../data/IGNITE/images/pdl1/pdl1/'\n",
    "\n",
    "label_files = sorted(glob.glob(os.path.join(label_dir, '*.json')))\n",
    "print(f\"ë‘ ë²ˆì§¸ ë°ì´í„°ì…‹ì—ì„œ ì°¾ì€ ë¼ë²¨ íŒŒì¼: {len(label_files)}ê°œ\")\n",
    "\n",
    "for label_file in label_files:\n",
    "    with open(label_file) as f:\n",
    "        data1 = json.load(f)\n",
    "    img_path = os.path.join(image_dir, data1['image']['file_name'])\n",
    "    if os.path.exists(img_path):\n",
    "        filenames.append(img_path)\n",
    "        temp_labels = []\n",
    "        for i in range(len(data1['annotations'])):\n",
    "            # ëª¨ë“  nucleusë¥¼ í´ë˜ìŠ¤ 1ë¡œ í†µì¼ (ì´ë¯¸ 1ë¡œ ì„¤ì •ë˜ì–´ ìˆìŒ)\n",
    "            temp_labels.append([1, int(data1['annotations'][i]['bbox'][0]),\n",
    "                         int(data1['annotations'][i]['bbox'][1]),int(data1['annotations'][i]['bbox'][2]),int(data1['annotations'][i]['bbox'][3])])\n",
    "        labels.append(temp_labels)\n",
    "\n",
    "print(f\"\\nì´ ë°ì´í„°:\")\n",
    "print(f\"ì´ë¯¸ì§€ íŒŒì¼: {len(filenames)}ê°œ\")\n",
    "print(f\"ë¼ë²¨ ì„¸íŠ¸: {len(labels)}ê°œ\")\n",
    "\n",
    "# ë¼ë²¨ ë¶„í¬ í™•ì¸\n",
    "total_labels = 0\n",
    "class_counts = {}\n",
    "for label_set in labels:\n",
    "    total_labels += len(label_set)\n",
    "    for label in label_set:\n",
    "        class_id = label[0]\n",
    "        if class_id not in class_counts:\n",
    "            class_counts[class_id] = 0\n",
    "        class_counts[class_id] += 1\n",
    "\n",
    "print(f\"\\në¼ë²¨ ë¶„í¬:\")\n",
    "print(f\"ì´ ë¼ë²¨ ìˆ˜: {total_labels}\")\n",
    "for class_id, count in sorted(class_counts.items()):\n",
    "    print(f\"í´ë˜ìŠ¤ {class_id}: {count}ê°œ\")\n",
    "\n",
    "# ë¹ˆ ë¼ë²¨ ì„¸íŠ¸ í™•ì¸\n",
    "empty_label_count = sum(1 for label_set in labels if len(label_set) == 0)\n",
    "print(f\"ë¹ˆ ë¼ë²¨ ì„¸íŠ¸: {empty_label_count}ê°œ\")\n",
    "\n",
    "if empty_label_count > 0:\n",
    "    print(\"Warning: ë¹ˆ ë¼ë²¨ ì„¸íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” lossê°€ 0ì´ ë˜ëŠ” ì›ì¸ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23c0666d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ë°ì´í„°: 479\n",
      "í›ˆë ¨ ë°ì´í„°: 431\n",
      "ê²€ì¦ ë°ì´í„°: 48\n",
      "\n",
      "ìƒ˜í”Œ ë°ì´í„° í™•ì¸:\n",
      "ì´ë¯¸ì§€ í¬ê¸°: torch.Size([3, 512, 512])\n",
      "í´ë˜ìŠ¤ ìˆ˜: 10\n",
      "ë°”ìš´ë”© ë°•ìŠ¤ ìˆ˜: 10\n",
      "í´ë˜ìŠ¤ ë²”ìœ„: 0 ~ 0\n",
      "ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ë²”ìœ„: x_center=0.010~0.887, y_center=0.025~0.936\n",
      "ë°”ìš´ë”© ë°•ìŠ¤ í¬ê¸° ë²”ìœ„: width=0.031~0.031, height=0.031~0.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.3'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/usr/local/lib/python3.12/dist-packages/albumentations/core/composition.py:251: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    }
   ],
   "source": [
    "class custom_dataset(data.Dataset):\n",
    "    def __init__(self, filenames, input_size, params, augment, labels=None, image_infos=None):\n",
    "        self.params = params\n",
    "        self.mosaic = augment\n",
    "        self.augment = augment\n",
    "        self.input_size = input_size\n",
    "        if labels is not None:\n",
    "            self.labels = labels\n",
    "            self.filenames = filenames\n",
    "            self.n = len(self.filenames)\n",
    "            self.image_infos = image_infos if image_infos is not None else [None]*len(filenames)\n",
    "        else:\n",
    "            loaded = self.load_label(filenames)\n",
    "            self.labels = list(loaded.values())\n",
    "            self.filenames = list(loaded.keys())\n",
    "            self.n = len(self.filenames)\n",
    "            self.image_infos = [None]*self.n\n",
    "        self.indices = range(self.n)\n",
    "        self.albumentations = Albumentations()\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    def __getitem__(self, index):\n",
    "        index = self.indices[index]\n",
    "        temp_label = copy.deepcopy(self.labels[index])\n",
    "        \n",
    "        image,crop_index=self.load_image(index)\n",
    "        \n",
    "        crop_x, crop_y = crop_index\n",
    "        label=[]\n",
    "        #bbox ì¢Œí‘œë¥¼ YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜: x_center, y_center, width, height (ëª¨ë‘ 0~1 ì •ê·œí™”)\n",
    "        for i in range(len(temp_label)):\n",
    "            x = temp_label[i][1]  # bbox x\n",
    "            y = temp_label[i][2]  # bbox y  \n",
    "            w = temp_label[i][3]  # bbox width\n",
    "            h = temp_label[i][4]  # bbox height\n",
    "            \n",
    "            # ë°”ìš´ë”© ë°•ìŠ¤ê°€ í¬ë¡­ëœ ì˜ì—­ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸ (ë” ê´€ëŒ€í•œ ì¡°ê±´)\n",
    "            # ë°”ìš´ë”© ë°•ìŠ¤ì˜ ì¤‘ì‹¬ì ì´ í¬ë¡­ ì˜ì—­ ë‚´ì— ìˆìœ¼ë©´ í¬í•¨\n",
    "            center_x = x + w/2\n",
    "            center_y = y + h/2\n",
    "            \n",
    "            if (center_x >= crop_x and center_y >= crop_y and \n",
    "                center_x <= crop_x + self.input_size and center_y <= crop_y + self.input_size):\n",
    "                \n",
    "                # YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜: (x_center, y_center, width, height) - ëª¨ë‘ 0~1 ì‚¬ì´ ì •ê·œí™”\n",
    "                norm_x_center = (center_x - crop_x) / self.input_size\n",
    "                norm_y_center = (center_y - crop_y) / self.input_size\n",
    "                norm_width = w / self.input_size\n",
    "                norm_height = h / self.input_size\n",
    "                \n",
    "                # ì •ê·œí™”ëœ ê°’ë“¤ì´ ìœ íš¨í•œ ë²”ìœ„ì— ìˆëŠ”ì§€ í™•ì¸\n",
    "                if (0 <= norm_x_center <= 1 and 0 <= norm_y_center <= 1 and \n",
    "                    norm_width > 0 and norm_height > 0):\n",
    "                    \n",
    "                    # YOLO í˜•ì‹: [class, x_center, y_center, width, height]\n",
    "                    converted_label = [temp_label[i][0], norm_x_center, norm_y_center, norm_width, norm_height]\n",
    "                    label.append(converted_label)\n",
    "\n",
    "        # ë””ë²„ê¹… ì •ë³´ ì¶”ê°€\n",
    "        # if len(label) == 0 and len(temp_label) > 0:\n",
    "        #     print(f\"Warning: íŒŒì¼ {self.filenames[index]}ì—ì„œ í¬ë¡­ í›„ ë¼ë²¨ì´ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ë¼ë²¨ ìˆ˜: {len(temp_label)}\")\n",
    "        #     print(f\"í¬ë¡­ ì •ë³´: crop_x={crop_x}, crop_y={crop_y}, input_size={self.input_size}\")\n",
    "\n",
    "        cls=[]\n",
    "        box=[]\n",
    "        for i in range(len(label)):\n",
    "            cls.append(label[i][0])\n",
    "            box.append(label[i][1:5])\n",
    "        \n",
    "        # í´ë˜ìŠ¤ ì¸ë±ìŠ¤ë¥¼ 0ë¶€í„° ì‹œì‘í•˜ë„ë¡ ë³€ê²½ (1 -> 0)\n",
    "        cls=np.array(cls, dtype=np.float32)\n",
    "        if len(cls) > 0:\n",
    "            cls = cls - 1  # 1 -> 0 ë³€í™˜ (ë‹¨ì¼ í´ë˜ìŠ¤ detection)\n",
    "            cls = np.clip(cls, 0, len(self.params['names'])-1)  # ìœ íš¨ ë²”ìœ„ë¡œ í´ë¦¬í•‘\n",
    "        \n",
    "        box=np.array(box, dtype=np.float32)\n",
    "        nl = len(box)\n",
    "        \n",
    "        if self.augment and nl > 0:  # ë¼ë²¨ì´ ìˆì„ ë•Œë§Œ augmentation ì ìš©\n",
    "            # Flip up-down\n",
    "            if random.random() < self.params['flip_ud']:\n",
    "                image = np.flipud(image).copy()\n",
    "                if nl:\n",
    "                    box[:, 1] = 1 - box[:, 1]  # y_center ë°˜ì „\n",
    "            # Flip left-right\n",
    "            if random.random() < self.params['flip_lr']:\n",
    "                image = np.fliplr(image).copy()\n",
    "                if nl:\n",
    "                    box[:, 0] = 1 - box[:, 0]  # x_center ë°˜ì „\n",
    "\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        \n",
    "        # ë¹ˆ í…ì„œ ëŒ€ì‹  ì ì ˆí•œ í¬ê¸°ì˜ ì¸ë±ìŠ¤ í…ì„œ ë°˜í™˜\n",
    "        return (torch.from_numpy(image).float(), \n",
    "                torch.from_numpy(cls).long(), \n",
    "                torch.from_numpy(box).float(), \n",
    "                torch.full((nl,), index, dtype=torch.long))\n",
    "\n",
    "    def load_image(self, i):\n",
    "        image = cv2.imread(self.filenames[i])\n",
    "        if image is None:\n",
    "            raise ValueError(f\"ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.filenames[i]}\")\n",
    "            \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGR -> RGB ë³€í™˜\n",
    "        h, w = image.shape[:2]\n",
    "        r = self.input_size / min(h, w)\n",
    "        \n",
    "        # ì´ë¯¸ì§€ê°€ input_sizeë³´ë‹¤ í° ê²½ìš° (r < 1) -> ëœë¤ í¬ë¡­\n",
    "        if r < 1:\n",
    "            # ì•ˆì „í•˜ê²Œ í¬ë¡­ ë²”ìœ„ ê³„ì‚°\n",
    "            max_h = max(0, h - self.input_size)\n",
    "            max_w = max(0, w - self.input_size)\n",
    "            h1 = random.randint(0, max_h) if max_h > 0 else 0\n",
    "            w1 = random.randint(0, max_w) if max_w > 0 else 0\n",
    "            image = image[h1:h1 + self.input_size, w1:w1 + self.input_size]\n",
    "        else:\n",
    "            # ì´ë¯¸ì§€ê°€ input_sizeë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ì€ ê²½ìš° (r >= 1) -> íŒ¨ë”©\n",
    "            h1 = 0\n",
    "            w1 = 0\n",
    "            pad_image = np.ones((self.input_size, self.input_size, 3), dtype=np.uint8)*255  # í°ìƒ‰ íŒ¨ë”©\n",
    "            pad_image[:min(h,self.input_size), :min(w,self.input_size), :] = image[:min(h,self.input_size), :min(w,self.input_size), :]\n",
    "            image = pad_image\n",
    "        return image, (w1, h1)  # x, y ìˆœì„œë¡œ ë°˜í™˜\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class Albumentations:\n",
    "    def __init__(self):\n",
    "        self.transform = None\n",
    "        try:\n",
    "            import albumentations\n",
    "\n",
    "            transforms = [albumentations.Blur(p=0.01),\n",
    "                          albumentations.CLAHE(p=0.01),\n",
    "                          albumentations.ToGray(p=0.01),\n",
    "                          albumentations.MedianBlur(p=0.01)]\n",
    "            self.transform = albumentations.Compose(transforms,\n",
    "                                                    albumentations.BboxParams('yolo', ['class_labels']))\n",
    "\n",
    "        except ImportError:  # package not installed, skip\n",
    "            pass\n",
    "\n",
    "    def __call__(self, image, box, cls):\n",
    "        if self.transform:\n",
    "            x = self.transform(image=image,\n",
    "                               bboxes=box,\n",
    "                               class_labels=cls)\n",
    "            image = x['image']\n",
    "            box = np.array(x['bboxes'])\n",
    "            cls = np.array(x['class_labels'])\n",
    "        return image, box, cls\n",
    "\n",
    "split=[0.9, 0.1]\n",
    "train_dataset=custom_dataset(filenames[:int(len(filenames)*split[0])],512, params, augment=True, labels=labels[:int(len(filenames)*split[0])])\n",
    "val_dataset = custom_dataset(filenames[int(len(filenames)*split[0]):],512, params, augment=False, labels=labels[int(len(filenames)*split[0]):])\n",
    "\n",
    "# ë°ì´í„°ì…‹ ê²€ì¦\n",
    "print(f\"ì „ì²´ ë°ì´í„°: {len(filenames)}\")\n",
    "print(f\"í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}\")\n",
    "print(f\"ê²€ì¦ ë°ì´í„°: {len(val_dataset)}\")\n",
    "\n",
    "# ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
    "sample_idx = 0\n",
    "sample_image, sample_cls, sample_box, sample_indices = train_dataset[sample_idx]\n",
    "print(f\"\\nìƒ˜í”Œ ë°ì´í„° í™•ì¸:\")\n",
    "print(f\"ì´ë¯¸ì§€ í¬ê¸°: {sample_image.shape}\")\n",
    "print(f\"í´ë˜ìŠ¤ ìˆ˜: {len(sample_cls)}\")\n",
    "print(f\"ë°”ìš´ë”© ë°•ìŠ¤ ìˆ˜: {len(sample_box)}\")\n",
    "if len(sample_cls) > 0:\n",
    "    print(f\"í´ë˜ìŠ¤ ë²”ìœ„: {sample_cls.min():.0f} ~ {sample_cls.max():.0f}\")\n",
    "if len(sample_box) > 0:\n",
    "    print(f\"ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ë²”ìœ„: x_center={sample_box[:, 0].min():.3f}~{sample_box[:, 0].max():.3f}, y_center={sample_box[:, 1].min():.3f}~{sample_box[:, 1].max():.3f}\")\n",
    "    print(f\"ë°”ìš´ë”© ë°•ìŠ¤ í¬ê¸° ë²”ìœ„: width={sample_box[:, 2].min():.3f}~{sample_box[:, 2].max():.3f}, height={sample_box[:, 3].min():.3f}~{sample_box[:, 3].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872981f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì°¸ê³ : ë°ì´í„°ì…‹ í¬ê¸°(431)ê°€ ë°°ì¹˜ í¬ê¸°(4)ë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "ë§ˆì§€ë§‰ ë°°ì¹˜ëŠ” 3ê°œì˜ ìƒ˜í”Œì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
      "ìµœì¢… í›ˆë ¨ ë°°ì¹˜ í¬ê¸°: 4\n",
      "ìµœì¢… ê²€ì¦ ë°°ì¹˜ í¬ê¸°: 1\n"
     ]
    }
   ],
   "source": [
    "def collate_fn1(batch):\n",
    "    samples, cls, box, indices = zip(*batch)\n",
    "\n",
    "    cls = torch.cat(cls, dim=0)\n",
    "    box = torch.cat(box, dim=0)\n",
    "\n",
    "    new_indices = list(indices)\n",
    "    for i in range(len(indices)):\n",
    "        new_indices[i] += i\n",
    "    indices = torch.cat(new_indices, dim=0)\n",
    "\n",
    "    targets = {'cls': cls,\n",
    "                'box': box,\n",
    "                'idx': indices}\n",
    "    return torch.stack(samples, dim=0), targets\n",
    "\n",
    "\n",
    "# ëª¨ë¸ ë° íŒŒë¼ë¯¸í„° ì¤€ë¹„\n",
    "model = nn.yolo_v11_m(len(params['names'])).to(device)\n",
    "optimizer = torch.optim.SGD(util.set_params(model, params['weight_decay']),\n",
    "                            params['max_lr'], params['momentum'], nesterov=True)\n",
    "criterion = util.ComputeLoss(model, params)\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë“œ (ì•ˆì „í•œ í•¨ìˆ˜ ì‚¬ìš©)\n",
    "batch_size = 4\n",
    "# ì•ˆì „í•˜ê²Œ ë°ì´í„°ë¡œë” ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "def create_safe_loader(dataset, batch_size, is_train=True):\n",
    "    \"\"\"\n",
    "    ë°°ì¹˜ í¬ê¸°ì— ë§ê²Œ ë°ì´í„°ì…‹ì„ ì¡°ì •í•˜ì—¬ ì•ˆì „í•˜ê²Œ ë°ì´í„°ë¡œë”ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    dataset_size = len(dataset)\n",
    "    \n",
    "    # ë°°ì¹˜ í¬ê¸°ê°€ ë°ì´í„°ì…‹ í¬ê¸°ë³´ë‹¤ í° ê²½ìš° ë°°ì¹˜ í¬ê¸° ì¡°ì •\n",
    "    if dataset_size < batch_size:\n",
    "        print(f\"ê²½ê³ : ë°ì´í„°ì…‹ í¬ê¸°({dataset_size})ê°€ ë°°ì¹˜ í¬ê¸°({batch_size})ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ {dataset_size}ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.\")\n",
    "        actual_batch_size = max(1, dataset_size)\n",
    "    else:\n",
    "        actual_batch_size = batch_size\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ì´ ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ëŠ”ì§€ í™•ì¸\n",
    "    if dataset_size % actual_batch_size != 0:\n",
    "        print(f\"ì°¸ê³ : ë°ì´í„°ì…‹ í¬ê¸°({dataset_size})ê°€ ë°°ì¹˜ í¬ê¸°({actual_batch_size})ë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        print(f\"ë§ˆì§€ë§‰ ë°°ì¹˜ëŠ” {dataset_size % actual_batch_size}ê°œì˜ ìƒ˜í”Œì„ í¬í•¨í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ë°ì´í„°ë¡œë” ìƒì„±\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=actual_batch_size, \n",
    "        shuffle=is_train,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn1,\n",
    "        drop_last=(not is_train)  # í›ˆë ¨ ì‹œì—ëŠ” ë§ˆì§€ë§‰ ë°°ì¹˜ ìœ ì§€, ê²€ì¦ ì‹œì—ëŠ” ë§ˆì§€ë§‰ ë°°ì¹˜ ì œì™¸\n",
    "    )\n",
    "    \n",
    "    return loader, actual_batch_size\n",
    "# ì•ˆì „í•˜ê²Œ ë°ì´í„°ë¡œë” ìƒì„±\n",
    "loader, train_batch_size = create_safe_loader(train_dataset, batch_size, is_train=True)\n",
    "val_loader, val_batch_size = create_safe_loader(val_dataset, 1, is_train=False)\n",
    "\n",
    "print(f\"ìµœì¢… í›ˆë ¨ ë°°ì¹˜ í¬ê¸°: {train_batch_size}\")\n",
    "print(f\"ìµœì¢… ê²€ì¦ ë°°ì¹˜ í¬ê¸°: {val_batch_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3682b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient accumulation steps: 16\n",
      "ë‹¨ì¼ í´ë˜ìŠ¤ detection: cls lossëŠ” 0.500ë¡œ ì„¤ì •ë¨\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000 Training:   0%|          | 0/108 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3611.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Epoch 1/10000 | Memory: 1.927G | Box: 0.002 | Cls: 19.764 | DFL: 0.001: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:13<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10000 Results:\n",
      "  Train Loss - Box: 0.0022, Cls: 19.7636, DFL: 0.0013, Total: 19.7671\n",
      "  Validation - Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000\n",
      "  mAP@0.5: 0.0000, mAP@0.5:0.95: 0.0000\n",
      "  Cohen's Kappa: 0.0000 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10000 | Memory: 1.927G | Box: 0.001 | Cls: 15.582 | DFL: 0.001: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:11<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10000 Results:\n",
      "  Train Loss - Box: 0.0010, Cls: 15.5815, DFL: 0.0005, Total: 15.5830\n",
      "  Validation - Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000\n",
      "  mAP@0.5: 0.0000, mAP@0.5:0.95: 0.0000\n",
      "  Cohen's Kappa: 0.0000 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10000 | Memory: 2.257G | Box: 0.002 | Cls: 10.042 | DFL: 0.001: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:12<00:00,  8.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10000 Results:\n",
      "  Train Loss - Box: 0.0021, Cls: 10.0424, DFL: 0.0012, Total: 10.0457\n",
      "  Validation - Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000\n",
      "  mAP@0.5: 0.0000, mAP@0.5:0.95: 0.0000\n",
      "  Cohen's Kappa: 0.0000 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10000 | Memory: 2.257G | Box: 0.000 | Cls: 5.178 | DFL: 0.000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:11<00:00,  9.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10000 Results:\n",
      "  Train Loss - Box: 0.0004, Cls: 5.1783, DFL: 0.0003, Total: 5.1789\n",
      "  Validation - Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000\n",
      "  mAP@0.5: 0.0000, mAP@0.5:0.95: 0.0000\n",
      "  Cohen's Kappa: 0.0000 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10000 | Memory: 2.257G | Box: 0.000 | Cls: 2.576 | DFL: 0.000:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 66/108 [00:07<00:04, 10.13it/s]"
     ]
    }
   ],
   "source": [
    "from utils.valid import compute_validation_metrics, compute_validation_metrics_with_kappa, get_kappa_interpretation\n",
    "from utils.valid import visualize_ground_truth_and_prediction_separately\n",
    "from utils.valid import plot_training_progress\n",
    "\n",
    "\n",
    "# main.pyì˜ train í•¨ìˆ˜ë¥¼ ì°¸ì¡°í•œ ê°œì„ ëœ í•™ìŠµ ë£¨í”„\n",
    "train_losses = []\n",
    "val_maps = []\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_map50s = []\n",
    "val_kappas = []  # Cohen's Kappa ì¶”ê°€\n",
    "epochs = 10000\n",
    "\n",
    "# ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "save_dir = '../../model/binary_yolov11/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "#ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° \n",
    "# checkpoint_path = os.path.join(save_dir, 'best_model.pt')\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     checkpoint = torch.load(checkpoint_path, map_location=device,weights_only=False)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "# main.py ìŠ¤íƒ€ì¼ì˜ ì„¤ì •ë“¤\n",
    "best_map = 0\n",
    "accumulate = max(round(64 / batch_size), 1)  # gradient accumulation steps\n",
    "amp_scale = torch.amp.GradScaler()  # mixed precision scaler\n",
    "\n",
    "print(f\"Gradient accumulation steps: {accumulate}\")\n",
    "print(f\"ë‹¨ì¼ í´ë˜ìŠ¤ detection: cls lossëŠ” {params['cls']:.3f}ë¡œ ì„¤ì •ë¨\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # í›ˆë ¨\n",
    "    model.train()\n",
    "    \n",
    "    # main.py ìŠ¤íƒ€ì¼ì˜ í‰ê·  ì†ì‹¤ ì¶”ì \n",
    "    avg_box_loss = util.AverageMeter()\n",
    "    avg_cls_loss = util.AverageMeter()\n",
    "    avg_dfl_loss = util.AverageMeter()\n",
    "    \n",
    "    train_pbar = tqdm.tqdm(enumerate(loader), total=len(loader), desc=f'Epoch {epoch+1}/{epochs} Training')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, (images, targets) in train_pbar:\n",
    "        step = i + len(loader) * epoch\n",
    "        \n",
    "        # íƒ€ê²Ÿì´ ë¹„ì–´ìˆëŠ” ë°°ì¹˜ ê±´ë„ˆë›°ê¸°\n",
    "        if len(targets['cls']) == 0:\n",
    "            print(f\"Warning: ë°°ì¹˜ {i}ì— íƒ€ê²Ÿì´ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "            continue\n",
    "            \n",
    "        images = images.to(device).float() / 255\n",
    "        \n",
    "        # íƒ€ê²Ÿì„ GPUë¡œ ì´ë™\n",
    "        targets['cls'] = targets['cls'].to(device)\n",
    "        targets['box'] = targets['box'].to(device)\n",
    "        targets['idx'] = targets['idx'].to(device)\n",
    "        \n",
    "        # íƒ€ê²Ÿ ê²€ì¦\n",
    "        valid_indices = (targets['cls'] >= 0) & (targets['cls'] < len(params['names']))\n",
    "        if not valid_indices.all():\n",
    "            print(f\"Warning: ìœ íš¨í•˜ì§€ ì•Šì€ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ë°œê²¬: {targets['cls'][~valid_indices]}\")\n",
    "            # ìœ íš¨í•œ íƒ€ê²Ÿë§Œ í•„í„°ë§\n",
    "            targets['cls'] = targets['cls'][valid_indices]\n",
    "            targets['box'] = targets['box'][valid_indices]\n",
    "            targets['idx'] = targets['idx'][valid_indices]\n",
    "        \n",
    "        # ë‹¤ì‹œ ë¹ˆ íƒ€ê²Ÿ ì²´í¬\n",
    "        if len(targets['cls']) == 0:\n",
    "            print(f\"Warning: í•„í„°ë§ í›„ ë°°ì¹˜ {i}ì— ìœ íš¨í•œ íƒ€ê²Ÿì´ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "            continue\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(images)\n",
    "            loss_box, loss_cls, loss_dfl = criterion(outputs, targets)\n",
    "        \n",
    "        # Loss ê²€ì¦ - NaNì´ë‚˜ ë¬´í•œëŒ€ ê°’ ì²´í¬\n",
    "        if torch.isnan(loss_box) or torch.isinf(loss_box):\n",
    "            print(f\"Warning: Box loss is NaN or Inf at step {step}\")\n",
    "            continue\n",
    "        if torch.isnan(loss_cls) or torch.isinf(loss_cls):\n",
    "            print(f\"Warning: Cls loss is NaN or Inf at step {step}\")\n",
    "            continue\n",
    "        if torch.isnan(loss_dfl) or torch.isinf(loss_dfl):\n",
    "            print(f\"Warning: DFL loss is NaN or Inf at step {step}\")\n",
    "            continue\n",
    "            \n",
    "        # í‰ê·  ì†ì‹¤ ì—…ë°ì´íŠ¸\n",
    "        avg_box_loss.update(loss_box.item(), images.size(0))\n",
    "        avg_cls_loss.update(loss_cls.item(), images.size(0))\n",
    "        avg_dfl_loss.update(loss_dfl.item(), images.size(0))\n",
    "        \n",
    "        # Loss scaling - ì˜¬ë°”ë¥¸ ìŠ¤ì¼€ì¼ë§\n",
    "        total_loss = (loss_box + loss_cls + loss_dfl) / accumulate\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        amp_scale.scale(total_loss).backward()\n",
    "        \n",
    "        # Gradient accumulation ë° optimization\n",
    "        if (step + 1) % accumulate == 0:\n",
    "            # Gradient clipping ì¶”ê°€\n",
    "            amp_scale.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            \n",
    "            # Optimization step\n",
    "            amp_scale.step(optimizer)\n",
    "            amp_scale.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # GPU ë©”ëª¨ë¦¬ ë™ê¸°í™”\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # ì§„í–‰ë¥  í‘œì‹œ ì—…ë°ì´íŠ¸ - cls lossê°€ 0ì´ë¯€ë¡œ ê°„ì†Œí™”\n",
    "        if torch.cuda.is_available():\n",
    "            memory = f'{torch.cuda.memory_reserved() / 1E9:.4g}G'\n",
    "        else:\n",
    "            memory = 'N/A'\n",
    "        \n",
    "        # cls lossê°€ 0ì´ë©´ í‘œì‹œí•˜ì§€ ì•ŠìŒ\n",
    "        if params['cls'] > 0:\n",
    "            s = f'Memory: {memory} | Box: {avg_box_loss.avg:.3f} | Cls: {avg_cls_loss.avg:.3f} | DFL: {avg_dfl_loss.avg:.3f}'\n",
    "        else:\n",
    "            s = f'Memory: {memory} | Box: {avg_box_loss.avg:.3f} | DFL: {avg_dfl_loss.avg:.3f} (ë‹¨ì¼í´ë˜ìŠ¤)'\n",
    "        train_pbar.set_description(f'Epoch {epoch+1}/{epochs} | {s}')\n",
    "    \n",
    "    # ì—í­ í‰ê·  ì†ì‹¤ ê³„ì‚°\n",
    "    avg_train_loss = avg_box_loss.avg + avg_cls_loss.avg + avg_dfl_loss.avg\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # ê²€ì¦ (Cohen's Kappa í¬í•¨)\n",
    "    precision, recall, map50, mean_ap, kappa = compute_validation_metrics_with_kappa(\n",
    "        model, val_loader, device, params\n",
    "    )\n",
    "    val_maps.append(mean_ap)\n",
    "    val_precisions.append(precision)\n",
    "    val_recalls.append(recall)\n",
    "    val_map50s.append(map50)\n",
    "    val_kappas.append(kappa)\n",
    "    \n",
    "    # F1-score ê³„ì‚°\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥ - cls lossê°€ 0ì´ë©´ ê°„ì†Œí™”\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs} Results:\")\n",
    "    if params['cls'] > 0:\n",
    "        print(f\"  Train Loss - Box: {avg_box_loss.avg:.4f}, Cls: {avg_cls_loss.avg:.4f}, DFL: {avg_dfl_loss.avg:.4f}, Total: {avg_train_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"  Train Loss - Box: {avg_box_loss.avg:.4f}, DFL: {avg_dfl_loss.avg:.4f}, Total: {avg_train_loss:.4f} (ë‹¨ì¼í´ë˜ìŠ¤)\")\n",
    "    print(f\"  Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}\")\n",
    "    print(f\"  mAP@0.5: {map50:.4f}, mAP@0.5:0.95: {mean_ap:.4f}\")\n",
    "    print(f\"  Cohen's Kappa: {kappa:.4f} ({get_kappa_interpretation(kappa)})\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ (mAP ê¸°ì¤€)\n",
    "    if mean_ap > best_map:\n",
    "        best_map = mean_ap\n",
    "        save_checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'amp_scale_state_dict': amp_scale.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'box_loss': avg_box_loss.avg,\n",
    "            'cls_loss': avg_cls_loss.avg,\n",
    "            'dfl_loss': avg_dfl_loss.avg,\n",
    "            'map': mean_ap,\n",
    "            'map50': map50,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'kappa': kappa,\n",
    "            'params': params\n",
    "        }\n",
    "        torch.save(save_checkpoint, os.path.join(save_dir, 'best_model.pt'))\n",
    "        print(f\"ğŸ‰ ìƒˆë¡œìš´ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥! mAP: {mean_ap:.4f}, Kappa: {kappa:.4f}\")\n",
    "    \n",
    "    # ìµœì‹  ëª¨ë¸ë„ ì €ì¥ (main.py ìŠ¤íƒ€ì¼)\n",
    "    last_checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'amp_scale_state_dict': amp_scale.state_dict(),\n",
    "        'train_loss': avg_train_loss,\n",
    "        'box_loss': avg_box_loss.avg,\n",
    "        'cls_loss': avg_cls_loss.avg,\n",
    "        'dfl_loss': avg_dfl_loss.avg,\n",
    "        'map': mean_ap,\n",
    "        'map50': map50,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'kappa': kappa,\n",
    "        'params': params\n",
    "    }\n",
    "    torch.save(last_checkpoint, os.path.join(save_dir, 'last_model.pt'))\n",
    "    \n",
    "    # 100 ì—í­ë§ˆë‹¤ í•™ìŠµ ì§„í–‰ ê·¸ë˜í”„ ìƒì„± ë° ì €ì¥\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        try:\n",
    "            print(f\"\\nğŸ“Š Epoch {epoch+1} - í•™ìŠµ ì§„í–‰ ìƒí™© ê·¸ë˜í”„ ìƒì„± ì¤‘...\")\n",
    "            plot_training_progress(train_losses, val_maps, val_precisions, val_recalls, val_map50s, epoch+1, save_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"ê·¸ë˜í”„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    # ê°œì„ ëœ ê²€ì¦ ì´ë¯¸ì§€ ì‹œê°í™” (ë§¤ 10 ì—í­ë§ˆë‹¤) - ì‹¤ì œ ë¼ë²¨ê³¼ ì˜ˆì¸¡ ë¼ë²¨ì„ ë³„ë„ figureë¡œ í‘œì‹œ\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        try:\n",
    "            # ì—¬ëŸ¬ ìƒ˜í”Œì— ëŒ€í•´ ì‹œê°í™”\n",
    "            num_samples = 1 # ìƒ˜í”Œ ìˆ˜ë¥¼ 1ê°œ\n",
    "            for sample_idx in range(num_samples):\n",
    "                print(f\"\\nğŸ“Š Epoch {epoch+1} - ê²€ì¦ ìƒ˜í”Œ {sample_idx+1}/{num_samples}:\")\n",
    "                print(\"=\" * 60)\n",
    "                \n",
    "                # ì‹¤ì œ ë¼ë²¨ê³¼ ì˜ˆì¸¡ ë¼ë²¨ì„ ë³„ë„ figureë¡œ í‘œì‹œ\n",
    "                sample_idx = random.randint(0, len(val_dataset)-1)\n",
    "                visualize_ground_truth_and_prediction_separately(\n",
    "                    model, val_dataset, idx=sample_idx, \n",
    "                    epoch=epoch+1, save_dir=save_dir\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ì‹œê°í™” ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "print(\"ğŸ¯ í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"ìµœì¢… ë² ìŠ¤íŠ¸ mAP: {best_map:.4f}\")\n",
    "print(f\"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {save_dir}\")\n",
    "print(f\"ë² ìŠ¤íŠ¸ ëª¨ë¸: {os.path.join(save_dir, 'best_model.pt')}\")\n",
    "print(f\"ìµœì‹  ëª¨ë¸: {os.path.join(save_dir, 'last_model.pt')}\")\n",
    "\n",
    "# ìµœì¢… ì„±ëŠ¥ ìš”ì•½\n",
    "if val_kappas:\n",
    "    final_kappa = val_kappas[-1]\n",
    "    final_map = val_maps[-1]\n",
    "    final_precision = val_precisions[-1]\n",
    "    final_recall = val_recalls[-1]\n",
    "    final_f1 = 2 * (final_precision * final_recall) / (final_precision + final_recall) if (final_precision + final_recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ìµœì¢… ì„±ëŠ¥ ìš”ì•½:\")\n",
    "    print(f\"  mAP@0.5:0.95: {final_map:.4f}\")\n",
    "    print(f\"  Cohen's Kappa: {final_kappa:.4f} ({get_kappa_interpretation(final_kappa)})\")\n",
    "    print(f\"  F1-score: {final_f1:.4f}\")\n",
    "    print(f\"  Precision: {final_precision:.4f}\")\n",
    "    print(f\"  Recall: {final_recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
