{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f827039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "import yaml\n",
    "from torch.utils import data\n",
    "# 개별 json 라벨 파일을 이용해 학습 데이터 리스트 생성\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from nets import nn\n",
    "from utils import util\n",
    "from utils.dataset import Dataset\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e71dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 및 데이터 경로 설정\n",
    "with open('utils/Binary_args.yaml', errors='ignore') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "print(\"모델 파라미터:\")\n",
    "print(f\"클래스 이름: {params['names']}\")\n",
    "print(f\"클래스 수: {len(params['names'])}\")\n",
    "\n",
    "label_dir = '../../data/IGNITE/annotations/pdl1/binary_individual/'\n",
    "image_dir = '../../data/IGNITE/images/pdl1/nuclei/'\n",
    "\n",
    "label_files = sorted(glob.glob(os.path.join(label_dir, '*.json')))\n",
    "filenames = []\n",
    "labels = []\n",
    "\n",
    "print(f\"첫 번째 데이터셋에서 찾은 라벨 파일: {len(label_files)}개\")\n",
    "\n",
    "for label_file in label_files:\n",
    "    with open(label_file) as f:\n",
    "        data1 = json.load(f)\n",
    "    img_path = os.path.join(image_dir, data1['image']['file_name'])\n",
    "    if os.path.exists(img_path):\n",
    "        filenames.append(img_path)\n",
    "        temp_labels = []\n",
    "        for i in range(len(data1['annotations'])):\n",
    "            # 모든 nucleus를 클래스 1로 통일\n",
    "            temp_labels.append([1, int(data1['annotations'][i]['bbox'][0]),\n",
    "                         int(data1['annotations'][i]['bbox'][1]),int(data1['annotations'][i]['bbox'][2]),int(data1['annotations'][i]['bbox'][3])])\n",
    "            \n",
    "        labels.append(temp_labels)\n",
    "        \n",
    "label_dir = '../../data/IGNITE/annotations/pdl1/individual/'\n",
    "image_dir = '../../data/IGNITE/images/pdl1/pdl1/'\n",
    "\n",
    "label_files = sorted(glob.glob(os.path.join(label_dir, '*.json')))\n",
    "print(f\"두 번째 데이터셋에서 찾은 라벨 파일: {len(label_files)}개\")\n",
    "\n",
    "for label_file in label_files:\n",
    "    with open(label_file) as f:\n",
    "        data1 = json.load(f)\n",
    "    img_path = os.path.join(image_dir, data1['image']['file_name'])\n",
    "    if os.path.exists(img_path):\n",
    "        filenames.append(img_path)\n",
    "        temp_labels = []\n",
    "        for i in range(len(data1['annotations'])):\n",
    "            # 모든 nucleus를 클래스 1로 통일 (이미 1로 설정되어 있음)\n",
    "            temp_labels.append([1, int(data1['annotations'][i]['bbox'][0]),\n",
    "                         int(data1['annotations'][i]['bbox'][1]),int(data1['annotations'][i]['bbox'][2]),int(data1['annotations'][i]['bbox'][3])])\n",
    "        labels.append(temp_labels)\n",
    "\n",
    "print(f\"\\n총 데이터:\")\n",
    "print(f\"이미지 파일: {len(filenames)}개\")\n",
    "print(f\"라벨 세트: {len(labels)}개\")\n",
    "\n",
    "# 라벨 분포 확인\n",
    "total_labels = 0\n",
    "class_counts = {}\n",
    "for label_set in labels:\n",
    "    total_labels += len(label_set)\n",
    "    for label in label_set:\n",
    "        class_id = label[0]\n",
    "        if class_id not in class_counts:\n",
    "            class_counts[class_id] = 0\n",
    "        class_counts[class_id] += 1\n",
    "\n",
    "print(f\"\\n라벨 분포:\")\n",
    "print(f\"총 라벨 수: {total_labels}\")\n",
    "for class_id, count in sorted(class_counts.items()):\n",
    "    print(f\"클래스 {class_id}: {count}개\")\n",
    "\n",
    "# 빈 라벨 세트 확인\n",
    "empty_label_count = sum(1 for label_set in labels if len(label_set) == 0)\n",
    "print(f\"빈 라벨 세트: {empty_label_count}개\")\n",
    "\n",
    "if empty_label_count > 0:\n",
    "    print(\"Warning: 빈 라벨 세트가 있습니다. 이는 loss가 0이 되는 원인이 될 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c0666d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.3'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/usr/local/lib/python3.12/dist-packages/albumentations/core/composition.py:251: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    }
   ],
   "source": [
    "class custom_dataset(data.Dataset):\n",
    "    def __init__(self, filenames, input_size, params, augment, labels=None, image_infos=None):\n",
    "        self.params = params\n",
    "        self.mosaic = augment\n",
    "        self.augment = augment\n",
    "        self.input_size = input_size\n",
    "        if labels is not None:\n",
    "            self.labels = labels\n",
    "            self.filenames = filenames\n",
    "            self.n = len(self.filenames)\n",
    "            self.image_infos = image_infos if image_infos is not None else [None]*len(filenames)\n",
    "        else:\n",
    "            loaded = self.load_label(filenames)\n",
    "            self.labels = list(loaded.values())\n",
    "            self.filenames = list(loaded.keys())\n",
    "            self.n = len(self.filenames)\n",
    "            self.image_infos = [None]*self.n\n",
    "        self.indices = range(self.n)\n",
    "        self.albumentations = Albumentations()\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    def __getitem__(self, index):\n",
    "        index = self.indices[index]\n",
    "        temp_label = copy.deepcopy(self.labels[index])\n",
    "        \n",
    "        image,crop_index=self.load_image(index)\n",
    "        \n",
    "        crop_x, crop_y = crop_index\n",
    "        label=[]\n",
    "        #bbox 좌표를 YOLO 형식으로 변환: x_center, y_center, width, height (모두 0~1 정규화)\n",
    "        for i in range(len(temp_label)):\n",
    "            x = temp_label[i][1]  # bbox x\n",
    "            y = temp_label[i][2]  # bbox y  \n",
    "            w = temp_label[i][3]  # bbox width\n",
    "            h = temp_label[i][4]  # bbox height\n",
    "            \n",
    "            # 바운딩 박스가 크롭된 영역 내에 있는지 확인 (더 관대한 조건)\n",
    "            # 바운딩 박스의 중심점이 크롭 영역 내에 있으면 포함\n",
    "            center_x = x + w/2\n",
    "            center_y = y + h/2\n",
    "            \n",
    "            if (center_x >= crop_x and center_y >= crop_y and \n",
    "                center_x <= crop_x + self.input_size and center_y <= crop_y + self.input_size):\n",
    "                \n",
    "                # YOLO 형식으로 변환: (x_center, y_center, width, height) - 모두 0~1 사이 정규화\n",
    "                norm_x_center = (center_x - crop_x) / self.input_size\n",
    "                norm_y_center = (center_y - crop_y) / self.input_size\n",
    "                norm_width = w / self.input_size\n",
    "                norm_height = h / self.input_size\n",
    "                \n",
    "                # 정규화된 값들이 유효한 범위에 있는지 확인\n",
    "                if (0 <= norm_x_center <= 1 and 0 <= norm_y_center <= 1 and \n",
    "                    norm_width > 0 and norm_height > 0):\n",
    "                    \n",
    "                    # YOLO 형식: [class, x_center, y_center, width, height]\n",
    "                    converted_label = [temp_label[i][0], norm_x_center, norm_y_center, norm_width, norm_height]\n",
    "                    label.append(converted_label)\n",
    "\n",
    "        # 디버깅 정보 추가\n",
    "        if len(label) == 0 and len(temp_label) > 0:\n",
    "            print(f\"Warning: 파일 {self.filenames[index]}에서 크롭 후 라벨이 없습니다. 원본 라벨 수: {len(temp_label)}\")\n",
    "            print(f\"크롭 정보: crop_x={crop_x}, crop_y={crop_y}, input_size={self.input_size}\")\n",
    "\n",
    "        cls=[]\n",
    "        box=[]\n",
    "        for i in range(len(label)):\n",
    "            cls.append(label[i][0])\n",
    "            box.append(label[i][1:5])\n",
    "        \n",
    "        # 클래스 인덱스를 0부터 시작하도록 변경 (1 -> 0)\n",
    "        cls=np.array(cls, dtype=np.float32)\n",
    "        if len(cls) > 0:\n",
    "            cls = cls - 1  # 1 -> 0 변환 (단일 클래스 detection)\n",
    "            cls = np.clip(cls, 0, len(self.params['names'])-1)  # 유효 범위로 클리핑\n",
    "        \n",
    "        box=np.array(box, dtype=np.float32)\n",
    "        nl = len(box)\n",
    "        \n",
    "        if self.augment and nl > 0:  # 라벨이 있을 때만 augmentation 적용\n",
    "            # Flip up-down\n",
    "            if random.random() < self.params['flip_ud']:\n",
    "                image = np.flipud(image).copy()\n",
    "                if nl:\n",
    "                    box[:, 1] = 1 - box[:, 1]  # y_center 반전\n",
    "            # Flip left-right\n",
    "            if random.random() < self.params['flip_lr']:\n",
    "                image = np.fliplr(image).copy()\n",
    "                if nl:\n",
    "                    box[:, 0] = 1 - box[:, 0]  # x_center 반전\n",
    "\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        \n",
    "        # 빈 텐서 대신 적절한 크기의 인덱스 텐서 반환\n",
    "        return (torch.from_numpy(image).float(), \n",
    "                torch.from_numpy(cls).long(), \n",
    "                torch.from_numpy(box).float(), \n",
    "                torch.full((nl,), index, dtype=torch.long))\n",
    "\n",
    "    def load_image(self, i):\n",
    "        image = cv2.imread(self.filenames[i])\n",
    "        if image is None:\n",
    "            raise ValueError(f\"이미지를 불러올 수 없습니다: {self.filenames[i]}\")\n",
    "            \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGR -> RGB 변환\n",
    "        h, w = image.shape[:2]\n",
    "        r = self.input_size / min(h, w)\n",
    "        \n",
    "        # 이미지가 input_size보다 큰 경우 (r < 1) -> 랜덤 크롭\n",
    "        if r < 1:\n",
    "            # 안전하게 크롭 범위 계산\n",
    "            max_h = max(0, h - self.input_size)\n",
    "            max_w = max(0, w - self.input_size)\n",
    "            h1 = random.randint(0, max_h) if max_h > 0 else 0\n",
    "            w1 = random.randint(0, max_w) if max_w > 0 else 0\n",
    "            image = image[h1:h1 + self.input_size, w1:w1 + self.input_size]\n",
    "        else:\n",
    "            # 이미지가 input_size보다 작거나 같은 경우 (r >= 1) -> 패딩\n",
    "            h1 = 0\n",
    "            w1 = 0\n",
    "            pad_image = np.ones((self.input_size, self.input_size, 3), dtype=np.uint8)*114  # 회색 패딩\n",
    "            pad_image[:min(h,self.input_size), :min(w,self.input_size), :] = image[:min(h,self.input_size), :min(w,self.input_size), :]\n",
    "            image = pad_image\n",
    "        return image, (w1, h1)  # x, y 순서로 반환\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class Albumentations:\n",
    "    def __init__(self):\n",
    "        self.transform = None\n",
    "        try:\n",
    "            import albumentations\n",
    "\n",
    "            transforms = [albumentations.Blur(p=0.01),\n",
    "                          albumentations.CLAHE(p=0.01),\n",
    "                          albumentations.ToGray(p=0.01),\n",
    "                          albumentations.MedianBlur(p=0.01)]\n",
    "            self.transform = albumentations.Compose(transforms,\n",
    "                                                    albumentations.BboxParams('yolo', ['class_labels']))\n",
    "\n",
    "        except ImportError:  # package not installed, skip\n",
    "            pass\n",
    "\n",
    "    def __call__(self, image, box, cls):\n",
    "        if self.transform:\n",
    "            x = self.transform(image=image,\n",
    "                               bboxes=box,\n",
    "                               class_labels=cls)\n",
    "            image = x['image']\n",
    "            box = np.array(x['bboxes'])\n",
    "            cls = np.array(x['class_labels'])\n",
    "        return image, box, cls\n",
    "\n",
    "split=[0.9, 0.1]\n",
    "train_dataset=custom_dataset(filenames[:int(len(filenames)*split[0])],512, params, augment=True, labels=labels[:int(len(filenames)*split[0])])\n",
    "val_dataset = custom_dataset(filenames[int(len(filenames)*split[0]):],512, params, augment=False, labels=labels[int(len(filenames)*split[0]):])\n",
    "\n",
    "# 데이터셋 검증\n",
    "print(f\"전체 데이터: {len(filenames)}\")\n",
    "print(f\"훈련 데이터: {len(train_dataset)}\")\n",
    "print(f\"검증 데이터: {len(val_dataset)}\")\n",
    "\n",
    "# 샘플 데이터 확인\n",
    "sample_idx = 0\n",
    "sample_image, sample_cls, sample_box, sample_indices = train_dataset[sample_idx]\n",
    "print(f\"\\n샘플 데이터 확인:\")\n",
    "print(f\"이미지 크기: {sample_image.shape}\")\n",
    "print(f\"클래스 수: {len(sample_cls)}\")\n",
    "print(f\"바운딩 박스 수: {len(sample_box)}\")\n",
    "if len(sample_cls) > 0:\n",
    "    print(f\"클래스 범위: {sample_cls.min():.0f} ~ {sample_cls.max():.0f}\")\n",
    "if len(sample_box) > 0:\n",
    "    print(f\"바운딩 박스 좌표 범위: x_center={sample_box[:, 0].min():.3f}~{sample_box[:, 0].max():.3f}, y_center={sample_box[:, 1].min():.3f}~{sample_box[:, 1].max():.3f}\")\n",
    "    print(f\"바운딩 박스 크기 범위: width={sample_box[:, 2].min():.3f}~{sample_box[:, 2].max():.3f}, height={sample_box[:, 3].min():.3f}~{sample_box[:, 3].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872981f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "참고: 데이터셋 크기(431)가 배치 크기(4)로 나누어 떨어지지 않습니다.\n",
      "마지막 배치는 3개의 샘플을 포함합니다.\n",
      "최종 훈련 배치 크기: 4\n",
      "최종 검증 배치 크기: 1\n"
     ]
    }
   ],
   "source": [
    "def collate_fn1(batch):\n",
    "    samples, cls, box, indices = zip(*batch)\n",
    "\n",
    "    cls = torch.cat(cls, dim=0)\n",
    "    box = torch.cat(box, dim=0)\n",
    "\n",
    "    new_indices = list(indices)\n",
    "    for i in range(len(indices)):\n",
    "        new_indices[i] += i\n",
    "    indices = torch.cat(new_indices, dim=0)\n",
    "\n",
    "    targets = {'cls': cls,\n",
    "                'box': box,\n",
    "                'idx': indices}\n",
    "    return torch.stack(samples, dim=0), targets\n",
    "\n",
    "\n",
    "# 모델 및 파라미터 준비\n",
    "model = nn.yolo_v11_m(len(params['names'])).to(device)\n",
    "optimizer = torch.optim.SGD(util.set_params(model, params['weight_decay']),\n",
    "                            params['min_lr'], params['momentum'], nesterov=True)\n",
    "criterion = util.ComputeLoss(model, params)\n",
    "\n",
    "# 데이터셋 및 데이터로드 (안전한 함수 사용)\n",
    "batch_size = 4\n",
    "# 안전하게 데이터로더 생성하는 함수\n",
    "def create_safe_loader(dataset, batch_size, is_train=True):\n",
    "    \"\"\"\n",
    "    배치 크기에 맞게 데이터셋을 조정하여 안전하게 데이터로더를 생성하는 함수\n",
    "    \"\"\"\n",
    "    dataset_size = len(dataset)\n",
    "    \n",
    "    # 배치 크기가 데이터셋 크기보다 큰 경우 배치 크기 조정\n",
    "    if dataset_size < batch_size:\n",
    "        print(f\"경고: 데이터셋 크기({dataset_size})가 배치 크기({batch_size})보다 작습니다. 배치 크기를 {dataset_size}로 조정합니다.\")\n",
    "        actual_batch_size = max(1, dataset_size)\n",
    "    else:\n",
    "        actual_batch_size = batch_size\n",
    "    \n",
    "    # 데이터셋이 배치 크기로 나누어 떨어지는지 확인\n",
    "    if dataset_size % actual_batch_size != 0:\n",
    "        print(f\"참고: 데이터셋 크기({dataset_size})가 배치 크기({actual_batch_size})로 나누어 떨어지지 않습니다.\")\n",
    "        print(f\"마지막 배치는 {dataset_size % actual_batch_size}개의 샘플을 포함합니다.\")\n",
    "    \n",
    "    # 데이터로더 생성\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=actual_batch_size, \n",
    "        shuffle=is_train,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn1,\n",
    "        drop_last=(not is_train)  # 훈련 시에는 마지막 배치 유지, 검증 시에는 마지막 배치 제외\n",
    "    )\n",
    "    \n",
    "    return loader, actual_batch_size\n",
    "# 안전하게 데이터로더 생성\n",
    "loader, train_batch_size = create_safe_loader(train_dataset, batch_size, is_train=True)\n",
    "val_loader, val_batch_size = create_safe_loader(val_dataset, 1, is_train=False)\n",
    "\n",
    "print(f\"최종 훈련 배치 크기: {train_batch_size}\")\n",
    "print(f\"최종 검증 배치 크기: {val_batch_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3682b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient accumulation steps: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000 Training:   0%|          | 0/108 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3611.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Epoch 1/10000 | Memory: 2.926G | Box: 3.008 | Cls: 8.548 | DFL: 1.232: 100%|██████████| 108/108 [00:13<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10000 Results:\n",
      "  Train Loss - Box: 3.0081, Cls: 8.5483, DFL: 1.2323, Total: 12.7887\n",
      "  Validation - Precision: 0.1011, Recall: 0.0037, F1-score: 0.0072\n",
      "  mAP@0.5: 0.0510, mAP@0.5:0.95: 0.0207\n",
      "  Cohen's Kappa: 0.0372 (Slight)\n",
      "--------------------------------------------------------------------------------\n",
      "🎉 새로운 베스트 모델 저장! mAP: 0.0207, Kappa: 0.0372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10000 | Memory: 2.926G | Box: 3.025 | Cls: 9.864 | DFL: 1.232: 100%|██████████| 108/108 [00:11<00:00,  9.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10000 Results:\n",
      "  Train Loss - Box: 3.0253, Cls: 9.8636, DFL: 1.2316, Total: 14.1205\n",
      "  Validation - Precision: 0.0799, Recall: 0.0029, F1-score: 0.0056\n",
      "  mAP@0.5: 0.0404, mAP@0.5:0.95: 0.0153\n",
      "  Cohen's Kappa: 0.0330 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10000 | Memory: 2.926G | Box: 3.036 | Cls: 11.432 | DFL: 1.244: 100%|██████████| 108/108 [00:11<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10000 Results:\n",
      "  Train Loss - Box: 3.0362, Cls: 11.4325, DFL: 1.2436, Total: 15.7122\n",
      "  Validation - Precision: 0.0960, Recall: 0.0043, F1-score: 0.0083\n",
      "  mAP@0.5: 0.0502, mAP@0.5:0.95: 0.0152\n",
      "  Cohen's Kappa: 0.0343 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10000 | Memory: 2.926G | Box: 3.027 | Cls: 6.738 | DFL: 1.239: 100%|██████████| 108/108 [00:11<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10000 Results:\n",
      "  Train Loss - Box: 3.0272, Cls: 6.7385, DFL: 1.2388, Total: 11.0045\n",
      "  Validation - Precision: 0.0586, Recall: 0.0011, F1-score: 0.0022\n",
      "  mAP@0.5: 0.0310, mAP@0.5:0.95: 0.0127\n",
      "  Cohen's Kappa: 0.0394 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10000 | Memory: 3.242G | Box: 3.038 | Cls: 7.249 | DFL: 1.241: 100%|██████████| 108/108 [00:12<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10000 Results:\n",
      "  Train Loss - Box: 3.0385, Cls: 7.2490, DFL: 1.2407, Total: 11.5282\n",
      "  Validation - Precision: 0.0792, Recall: 0.0037, F1-score: 0.0071\n",
      "  mAP@0.5: 0.0406, mAP@0.5:0.95: 0.0132\n",
      "  Cohen's Kappa: 0.0363 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10000 | Memory: 3.242G | Box: 3.020 | Cls: 10.374 | DFL: 1.235: 100%|██████████| 108/108 [00:11<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10000 Results:\n",
      "  Train Loss - Box: 3.0202, Cls: 10.3742, DFL: 1.2353, Total: 14.6296\n",
      "  Validation - Precision: 0.0688, Recall: 0.0028, F1-score: 0.0054\n",
      "  mAP@0.5: 0.0348, mAP@0.5:0.95: 0.0143\n",
      "  Cohen's Kappa: 0.0253 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10000 | Memory: 3.242G | Box: 3.045 | Cls: 6.807 | DFL: 1.241: 100%|██████████| 108/108 [00:12<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10000 Results:\n",
      "  Train Loss - Box: 3.0450, Cls: 6.8072, DFL: 1.2410, Total: 11.0932\n",
      "  Validation - Precision: 0.0843, Recall: 0.0039, F1-score: 0.0074\n",
      "  mAP@0.5: 0.0429, mAP@0.5:0.95: 0.0137\n",
      "  Cohen's Kappa: 0.0273 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10000 | Memory: 3.242G | Box: 3.034 | Cls: 7.401 | DFL: 1.235: 100%|██████████| 108/108 [00:12<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10000 Results:\n",
      "  Train Loss - Box: 3.0344, Cls: 7.4014, DFL: 1.2349, Total: 11.6706\n",
      "  Validation - Precision: 0.0915, Recall: 0.0054, F1-score: 0.0102\n",
      "  mAP@0.5: 0.0468, mAP@0.5:0.95: 0.0171\n",
      "  Cohen's Kappa: 0.0330 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10000 | Memory: 3.242G | Box: 3.016 | Cls: 8.758 | DFL: 1.232: 100%|██████████| 108/108 [00:13<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10000 Results:\n",
      "  Train Loss - Box: 3.0162, Cls: 8.7583, DFL: 1.2316, Total: 13.0062\n",
      "  Validation - Precision: 0.0666, Recall: 0.0039, F1-score: 0.0073\n",
      "  mAP@0.5: 0.0341, mAP@0.5:0.95: 0.0127\n",
      "  Cohen's Kappa: 0.0362 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10000 | Memory: 3.242G | Box: 3.044 | Cls: 8.575 | DFL: 1.243: 100%|██████████| 108/108 [00:11<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10000 Results:\n",
      "  Train Loss - Box: 3.0439, Cls: 8.5746, DFL: 1.2434, Total: 12.8620\n",
      "  Validation - Precision: 0.0849, Recall: 0.0061, F1-score: 0.0114\n",
      "  mAP@0.5: 0.0440, mAP@0.5:0.95: 0.0143\n",
      "  Cohen's Kappa: 0.0232 (Slight)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 10 - 검증 샘플 1/1:\n",
      "============================================================\n",
      "✅ 비교 이미지 저장: ../../model/binary_yolov11/validation_comparison_epoch_10.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/10000 | Memory: 3.242G | Box: 3.028 | Cls: 10.521 | DFL: 1.233: 100%|██████████| 108/108 [00:11<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/10000 Results:\n",
      "  Train Loss - Box: 3.0281, Cls: 10.5208, DFL: 1.2326, Total: 14.7815\n",
      "  Validation - Precision: 0.1195, Recall: 0.0029, F1-score: 0.0056\n",
      "  mAP@0.5: 0.0603, mAP@0.5:0.95: 0.0228\n",
      "  Cohen's Kappa: 0.0325 (Slight)\n",
      "--------------------------------------------------------------------------------\n",
      "🎉 새로운 베스트 모델 저장! mAP: 0.0228, Kappa: 0.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/10000 | Memory: 3.242G | Box: 3.031 | Cls: 7.507 | DFL: 1.233: 100%|██████████| 108/108 [00:12<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/10000 Results:\n",
      "  Train Loss - Box: 3.0313, Cls: 7.5068, DFL: 1.2335, Total: 11.7715\n",
      "  Validation - Precision: 0.0557, Recall: 0.0056, F1-score: 0.0102\n",
      "  mAP@0.5: 0.0282, mAP@0.5:0.95: 0.0099\n",
      "  Cohen's Kappa: 0.0313 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/10000 | Memory: 3.242G | Box: 3.043 | Cls: 9.600 | DFL: 1.246: 100%|██████████| 108/108 [00:12<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/10000 Results:\n",
      "  Train Loss - Box: 3.0433, Cls: 9.5999, DFL: 1.2460, Total: 13.8892\n",
      "  Validation - Precision: 0.1224, Recall: 0.0032, F1-score: 0.0063\n",
      "  mAP@0.5: 0.0658, mAP@0.5:0.95: 0.0194\n",
      "  Cohen's Kappa: 0.0206 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/10000 | Memory: 3.242G | Box: 3.034 | Cls: 8.566 | DFL: 1.236: 100%|██████████| 108/108 [00:12<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/10000 Results:\n",
      "  Train Loss - Box: 3.0338, Cls: 8.5656, DFL: 1.2362, Total: 12.8356\n",
      "  Validation - Precision: 0.0861, Recall: 0.0050, F1-score: 0.0094\n",
      "  mAP@0.5: 0.0445, mAP@0.5:0.95: 0.0157\n",
      "  Cohen's Kappa: 0.0355 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/10000 | Memory: 3.242G | Box: 3.029 | Cls: 12.273 | DFL: 1.238: 100%|██████████| 108/108 [00:12<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/10000 Results:\n",
      "  Train Loss - Box: 3.0285, Cls: 12.2729, DFL: 1.2376, Total: 16.5390\n",
      "  Validation - Precision: 0.0646, Recall: 0.0018, F1-score: 0.0034\n",
      "  mAP@0.5: 0.0331, mAP@0.5:0.95: 0.0090\n",
      "  Cohen's Kappa: 0.0300 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/10000 | Memory: 3.242G | Box: 3.037 | Cls: 10.325 | DFL: 1.246: 100%|██████████| 108/108 [00:11<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/10000 Results:\n",
      "  Train Loss - Box: 3.0365, Cls: 10.3248, DFL: 1.2464, Total: 14.6077\n",
      "  Validation - Precision: 0.0674, Recall: 0.0049, F1-score: 0.0091\n",
      "  mAP@0.5: 0.0340, mAP@0.5:0.95: 0.0098\n",
      "  Cohen's Kappa: 0.0331 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/10000 | Memory: 3.242G | Box: 3.036 | Cls: 10.257 | DFL: 1.237: 100%|██████████| 108/108 [00:12<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/10000 Results:\n",
      "  Train Loss - Box: 3.0361, Cls: 10.2574, DFL: 1.2371, Total: 14.5306\n",
      "  Validation - Precision: 0.0950, Recall: 0.0032, F1-score: 0.0062\n",
      "  mAP@0.5: 0.0478, mAP@0.5:0.95: 0.0176\n",
      "  Cohen's Kappa: 0.0413 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/10000 | Memory: 3.242G | Box: 3.029 | Cls: 9.319 | DFL: 1.239: 100%|██████████| 108/108 [00:11<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/10000 Results:\n",
      "  Train Loss - Box: 3.0288, Cls: 9.3188, DFL: 1.2395, Total: 13.5871\n",
      "  Validation - Precision: 0.0694, Recall: 0.0029, F1-score: 0.0056\n",
      "  mAP@0.5: 0.0367, mAP@0.5:0.95: 0.0167\n",
      "  Cohen's Kappa: 0.0355 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/10000 | Memory: 3.242G | Box: 3.040 | Cls: 6.820 | DFL: 1.234: 100%|██████████| 108/108 [00:12<00:00,  8.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/10000 Results:\n",
      "  Train Loss - Box: 3.0402, Cls: 6.8199, DFL: 1.2338, Total: 11.0939\n",
      "  Validation - Precision: 0.0798, Recall: 0.0063, F1-score: 0.0117\n",
      "  mAP@0.5: 0.0404, mAP@0.5:0.95: 0.0144\n",
      "  Cohen's Kappa: 0.0357 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/10000 | Memory: 3.244G | Box: 3.005 | Cls: 7.386 | DFL: 1.232: 100%|██████████| 108/108 [00:12<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/10000 Results:\n",
      "  Train Loss - Box: 3.0048, Cls: 7.3857, DFL: 1.2322, Total: 11.6227\n",
      "  Validation - Precision: 0.0985, Recall: 0.0037, F1-score: 0.0071\n",
      "  mAP@0.5: 0.0505, mAP@0.5:0.95: 0.0183\n",
      "  Cohen's Kappa: 0.0422 (Slight)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 20 - 검증 샘플 1/1:\n",
      "============================================================\n",
      "✅ 비교 이미지 저장: ../../model/binary_yolov11/validation_comparison_epoch_20.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/10000 | Memory: 3.244G | Box: 3.037 | Cls: 8.444 | DFL: 1.239: 100%|██████████| 108/108 [00:11<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/10000 Results:\n",
      "  Train Loss - Box: 3.0367, Cls: 8.4439, DFL: 1.2388, Total: 12.7194\n",
      "  Validation - Precision: 0.0724, Recall: 0.0029, F1-score: 0.0055\n",
      "  mAP@0.5: 0.0376, mAP@0.5:0.95: 0.0132\n",
      "  Cohen's Kappa: 0.0417 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/10000 | Memory: 3.244G | Box: 3.029 | Cls: 8.179 | DFL: 1.238: 100%|██████████| 108/108 [00:13<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/10000 Results:\n",
      "  Train Loss - Box: 3.0292, Cls: 8.1789, DFL: 1.2380, Total: 12.4462\n",
      "  Validation - Precision: 0.0625, Recall: 0.0022, F1-score: 0.0042\n",
      "  mAP@0.5: 0.0315, mAP@0.5:0.95: 0.0100\n",
      "  Cohen's Kappa: 0.0335 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/10000 | Memory: 3.244G | Box: 3.022 | Cls: 7.272 | DFL: 1.236: 100%|██████████| 108/108 [00:12<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/10000 Results:\n",
      "  Train Loss - Box: 3.0220, Cls: 7.2719, DFL: 1.2363, Total: 11.5302\n",
      "  Validation - Precision: 0.0739, Recall: 0.0010, F1-score: 0.0020\n",
      "  mAP@0.5: 0.0371, mAP@0.5:0.95: 0.0100\n",
      "  Cohen's Kappa: 0.0225 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/10000 | Memory: 3.244G | Box: 3.034 | Cls: 8.177 | DFL: 1.243: 100%|██████████| 108/108 [00:13<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/10000 Results:\n",
      "  Train Loss - Box: 3.0339, Cls: 8.1772, DFL: 1.2430, Total: 12.4541\n",
      "  Validation - Precision: 0.0780, Recall: 0.0053, F1-score: 0.0100\n",
      "  mAP@0.5: 0.0393, mAP@0.5:0.95: 0.0136\n",
      "  Cohen's Kappa: 0.0281 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/10000 | Memory: 3.274G | Box: 3.017 | Cls: 7.180 | DFL: 1.222: 100%|██████████| 108/108 [00:11<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/10000 Results:\n",
      "  Train Loss - Box: 3.0167, Cls: 7.1803, DFL: 1.2215, Total: 11.4185\n",
      "  Validation - Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000\n",
      "  mAP@0.5: 0.0000, mAP@0.5:0.95: 0.0000\n",
      "  Cohen's Kappa: 0.0000 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/10000 | Memory: 3.592G | Box: 0.000 | Cls: 0.000 | DFL: 0.000: 100%|██████████| 108/108 [00:11<00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/10000 Results:\n",
      "  Train Loss - Box: 0.0000, Cls: 0.0000, DFL: 0.0000, Total: 0.0000\n",
      "  Validation - Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000\n",
      "  mAP@0.5: 0.0000, mAP@0.5:0.95: 0.0000\n",
      "  Cohen's Kappa: 0.0000 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/10000 | Memory: 3.592G | Box: 0.000 | Cls: 0.000 | DFL: 0.000: 100%|██████████| 108/108 [00:11<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/10000 Results:\n",
      "  Train Loss - Box: 0.0000, Cls: 0.0000, DFL: 0.0000, Total: 0.0000\n",
      "  Validation - Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000\n",
      "  mAP@0.5: 0.0000, mAP@0.5:0.95: 0.0000\n",
      "  Cohen's Kappa: 0.0000 (Slight)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/10000 | Memory: 3.592G | Box: 0.000 | Cls: 0.000 | DFL: 0.000:  56%|█████▋    | 61/108 [00:06<00:05,  8.75it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Forward pass with mixed precision\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 50\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     loss_box, loss_cls, loss_dfl \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# 평균 손실 업데이트\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/IHC_biomarker/code/ihc_cell_count/nets/nn.py:295\u001b[0m, in \u001b[0;36mYOLO.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 295\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfpn(x)\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;28mlist\u001b[39m(x))\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/IHC_biomarker/code/ihc_cell_count/nets/nn.py:186\u001b[0m, in \u001b[0;36mDarkNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    184\u001b[0m p1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp1(x)\n\u001b[1;32m    185\u001b[0m p2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp2(p1)\n\u001b[0;32m--> 186\u001b[0m p3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m p4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp4(p3)\n\u001b[1;32m    188\u001b[0m p5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp5(p4)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/IHC_biomarker/code/ihc_cell_count/nets/nn.py:79\u001b[0m, in \u001b[0;36mCSP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     78\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 79\u001b[0m     \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mres_m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(torch\u001b[38;5;241m.\u001b[39mcat(y, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/IHC_biomarker/code/ihc_cell_count/nets/nn.py:79\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     78\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 79\u001b[0m     y\u001b[38;5;241m.\u001b[39mextend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_m)\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(torch\u001b[38;5;241m.\u001b[39mcat(y, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/IHC_biomarker/code/ihc_cell_count/nets/nn.py:62\u001b[0m, in \u001b[0;36mCSPModule.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 62\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(torch\u001b[38;5;241m.\u001b[39mcat((y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/IHC_biomarker/code/ihc_cell_count/nets/nn.py:36\u001b[0m, in \u001b[0;36mConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:432\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2380\u001b[0m, in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   2378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   2379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 2380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.valid import compute_validation_metrics, compute_validation_metrics_with_kappa, get_kappa_interpretation\n",
    "from utils.valid import visualize_ground_truth_and_prediction_separately\n",
    "from utils.valid import plot_training_progress\n",
    "\n",
    "\n",
    "# main.py의 train 함수를 참조한 개선된 학습 루프\n",
    "train_losses = []\n",
    "val_maps = []\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_map50s = []\n",
    "val_kappas = []  # Cohen's Kappa 추가\n",
    "epochs = 10000\n",
    "\n",
    "# 체크포인트 저장을 위한 디렉토리 생성\n",
    "save_dir = '../../model/binary_yolov11/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "#체크포인트 불러오기 \n",
    "# checkpoint_path = os.path.join(save_dir, 'best_model.pt')\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     checkpoint = torch.load(checkpoint_path, map_location=device,weights_only=False)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "# main.py 스타일의 설정들\n",
    "best_map = 0\n",
    "accumulate = max(round(64 / batch_size), 1)  # gradient accumulation steps\n",
    "amp_scale = torch.amp.GradScaler()  # mixed precision scaler\n",
    "\n",
    "print(f\"Gradient accumulation steps: {accumulate}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 훈련\n",
    "    model.train()\n",
    "    \n",
    "    # main.py 스타일의 평균 손실 추적\n",
    "    avg_box_loss = util.AverageMeter()\n",
    "    avg_cls_loss = util.AverageMeter()\n",
    "    avg_dfl_loss = util.AverageMeter()\n",
    "    \n",
    "    train_pbar = tqdm.tqdm(enumerate(loader), total=len(loader), desc=f'Epoch {epoch+1}/{epochs} Training')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, (images, targets) in train_pbar:\n",
    "        step = i + len(loader) * epoch\n",
    "        images = images.to(device).float() / 255\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(images)\n",
    "            loss_box, loss_cls, loss_dfl = criterion(outputs, targets)\n",
    "        \n",
    "        # 평균 손실 업데이트\n",
    "        avg_box_loss.update(loss_box.item(), images.size(0))\n",
    "        avg_cls_loss.update(loss_cls.item(), images.size(0))\n",
    "        avg_dfl_loss.update(loss_dfl.item(), images.size(0))\n",
    "        \n",
    "        # Loss scaling for gradient accumulation\n",
    "        loss_box *= batch_size  # loss scaled by batch_size\n",
    "        loss_cls *= batch_size  # loss scaled by batch_size  \n",
    "        loss_dfl *= batch_size  # loss scaled by batch_size\n",
    "        \n",
    "        total_loss = loss_box + loss_cls + loss_dfl\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        amp_scale.scale(total_loss).backward()\n",
    "        \n",
    "        # Gradient accumulation 및 optimization\n",
    "        if step % accumulate == 0:\n",
    "            # Gradient clipping 및 optimization\n",
    "            amp_scale.step(optimizer)\n",
    "            amp_scale.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # GPU 메모리 동기화\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # 진행률 표시 업데이트 (main.py 스타일)\n",
    "        memory = f'{torch.cuda.memory_reserved() / 1E9:.4g}G'\n",
    "        s = f'Memory: {memory} | Box: {avg_box_loss.avg:.3f} | Cls: {avg_cls_loss.avg:.3f} | DFL: {avg_dfl_loss.avg:.3f}'\n",
    "        train_pbar.set_description(f'Epoch {epoch+1}/{epochs} | {s}')\n",
    "    \n",
    "    # 에폭 평균 손실 계산\n",
    "    avg_train_loss = avg_box_loss.avg + avg_cls_loss.avg + avg_dfl_loss.avg\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # 검증 (Cohen's Kappa 포함)\n",
    "    precision, recall, map50, mean_ap, kappa = compute_validation_metrics_with_kappa(\n",
    "        model, val_loader, device, params\n",
    "    )\n",
    "    val_maps.append(mean_ap)\n",
    "    val_precisions.append(precision)\n",
    "    val_recalls.append(recall)\n",
    "    val_map50s.append(map50)\n",
    "    val_kappas.append(kappa)\n",
    "    \n",
    "    # F1-score 계산\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # 결과 출력 (Cohen's Kappa 포함)\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs} Results:\")\n",
    "    print(f\"  Train Loss - Box: {avg_box_loss.avg:.4f}, Cls: {avg_cls_loss.avg:.4f}, DFL: {avg_dfl_loss.avg:.4f}, Total: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}\")\n",
    "    print(f\"  mAP@0.5: {map50:.4f}, mAP@0.5:0.95: {mean_ap:.4f}\")\n",
    "    print(f\"  Cohen's Kappa: {kappa:.4f} ({get_kappa_interpretation(kappa)})\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # 베스트 모델 저장 (mAP 기준)\n",
    "    if mean_ap > best_map:\n",
    "        best_map = mean_ap\n",
    "        save_checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'amp_scale_state_dict': amp_scale.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'box_loss': avg_box_loss.avg,\n",
    "            'cls_loss': avg_cls_loss.avg,\n",
    "            'dfl_loss': avg_dfl_loss.avg,\n",
    "            'map': mean_ap,\n",
    "            'map50': map50,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'kappa': kappa,\n",
    "            'params': params\n",
    "        }\n",
    "        torch.save(save_checkpoint, os.path.join(save_dir, 'best_model.pt'))\n",
    "        print(f\"🎉 새로운 베스트 모델 저장! mAP: {mean_ap:.4f}, Kappa: {kappa:.4f}\")\n",
    "    \n",
    "    # 최신 모델도 저장 (main.py 스타일)\n",
    "    last_checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'amp_scale_state_dict': amp_scale.state_dict(),\n",
    "        'train_loss': avg_train_loss,\n",
    "        'box_loss': avg_box_loss.avg,\n",
    "        'cls_loss': avg_cls_loss.avg,\n",
    "        'dfl_loss': avg_dfl_loss.avg,\n",
    "        'map': mean_ap,\n",
    "        'map50': map50,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'kappa': kappa,\n",
    "        'params': params\n",
    "    }\n",
    "    torch.save(last_checkpoint, os.path.join(save_dir, 'last_model.pt'))\n",
    "    \n",
    "    # 100 에폭마다 학습 진행 그래프 생성 및 저장\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        try:\n",
    "            print(f\"\\n📊 Epoch {epoch+1} - 학습 진행 상황 그래프 생성 중...\")\n",
    "            plot_training_progress(train_losses, val_maps, val_precisions, val_recalls, val_map50s, epoch+1, save_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"그래프 생성 중 오류: {e}\")\n",
    "    \n",
    "    # 개선된 검증 이미지 시각화 (매 10 에폭마다) - 실제 라벨과 예측 라벨을 별도 figure로 표시\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        try:\n",
    "            # 여러 샘플에 대해 시각화\n",
    "            num_samples = 1 # 샘플 수를 1개\n",
    "            for sample_idx in range(num_samples):\n",
    "                print(f\"\\n📊 Epoch {epoch+1} - 검증 샘플 {sample_idx+1}/{num_samples}:\")\n",
    "                print(\"=\" * 60)\n",
    "                \n",
    "                # 실제 라벨과 예측 라벨을 별도 figure로 표시\n",
    "                sample_idx = random.randint(0, len(val_dataset)-1)\n",
    "                visualize_ground_truth_and_prediction_separately(\n",
    "                    model, val_dataset, idx=sample_idx, \n",
    "                    epoch=epoch+1, save_dir=save_dir\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"시각화 중 오류: {e}\")\n",
    "\n",
    "print(\"🎯 학습 완료!\")\n",
    "print(f\"최종 베스트 mAP: {best_map:.4f}\")\n",
    "print(f\"모델 저장 위치: {save_dir}\")\n",
    "print(f\"베스트 모델: {os.path.join(save_dir, 'best_model.pt')}\")\n",
    "print(f\"최신 모델: {os.path.join(save_dir, 'last_model.pt')}\")\n",
    "\n",
    "# 최종 성능 요약\n",
    "if val_kappas:\n",
    "    final_kappa = val_kappas[-1]\n",
    "    final_map = val_maps[-1]\n",
    "    final_precision = val_precisions[-1]\n",
    "    final_recall = val_recalls[-1]\n",
    "    final_f1 = 2 * (final_precision * final_recall) / (final_precision + final_recall) if (final_precision + final_recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n📊 최종 성능 요약:\")\n",
    "    print(f\"  mAP@0.5:0.95: {final_map:.4f}\")\n",
    "    print(f\"  Cohen's Kappa: {final_kappa:.4f} ({get_kappa_interpretation(final_kappa)})\")\n",
    "    print(f\"  F1-score: {final_f1:.4f}\")\n",
    "    print(f\"  Precision: {final_precision:.4f}\")\n",
    "    print(f\"  Recall: {final_recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
